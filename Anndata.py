# -*- coding: utf-8 -*-
"""Anndata.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/184y3IDPK-cTGR2HtMXIzKfElg0TrgeKT

**AnnData**

The materials are presented on :https://anndata.readthedocs.io/en/stable/tutorials/index.html
"""

# Install the required packages
!pip install scanpy anndata pooch

# Import and test
import scanpy as sc
import anndata as ad
import pooch

print("âœ… All packages installed successfully!")

import numpy as np
import pandas as pd
import anndata as ad
from scipy.sparse import csr_matrix
print(ad.__version__)

"""**Initializing AnnData**"""

counts = csr_matrix(np.random.poisson(1, size=(100, 2000)), dtype=np.float32)
adata = ad.AnnData(counts)
adata

adata.X

adata.obs_names = [f"Cell_{i:d}" for i in range(adata.n_obs)]
adata.var_names = [f"Gene_{i:d}" for i in range(adata.n_vars)]
print(adata.obs_names[:10])
print(adata.var_names[:10])


# Subsetting AnnData

adata[["Cell_1", "Cell_10"], ["Gene_5", "Gene_1900"]]

"""**Adding aligned metadata : Observation/Variable level**

"""

print(adata.obs.head(2))
print(adata.var.head(2))

ct = np.random.choice(["B", "T", "Monocyte"], size=(adata.n_obs,))
adata.obs["cell_type"] = pd.Categorical(ct)  # Categoricals are preferred for efficiency
adata.obs

"""**Subsetting using metadata**"""

bdata = adata[adata.obs.cell_type == "B"]
print("b cell data")
bdata

"""**Observation/variable-level matrices**"""

# AnnData has the .obsm/.varm attributes. We use keys to identify the different matrices we insert.
# The restriction of .obsm/.varm are that .obsm matrices must length equal to the number of observations as .n_obs and .varm matrices must length equal to .n_vars

adata.obsm["X_umap"] = np.random.normal(0, 1, size=(adata.n_obs, 2))
adata.varm["gene_stuff"] = np.random.normal(0, 1, size=(adata.n_vars, 5))
adata.obsm

adata

print(adata.n_obs) # (number of observations/cells) = 100
print(adata.n_vars) # (number of variables/genes) = 2000

# Display the first 5 rows of 'X_umap'
print("First 5 rows of adata.obsm['X_umap']:")
print(adata.obsm['X_umap'][:5, :])

# Display the first 5 values from the first column of 'X_umap'
print("\nFirst 5 values from the first column of adata.obsm['X_umap']:")
print(adata.obsm['X_umap'][:5, 0])

# Display the first 5 values from the second column of 'X_umap'
print("\nFirst 5 values from the second column of adata.obsm['X_umap']:")
print(adata.obsm['X_umap'][:5, 1])

# Here are several ways to view the first 4 rows and 4 columns of AnnData objects:
# Convert to DataFrame (Recommended)

# View first 4 rows and 4 columns of the expression matrix
adata.to_df().iloc[:4, :4]

# View first 4 rows and 4 columns of the expression matrix

# View raw expression data (first 4x4)
print("Expression matrix (4x4):")
print(adata.X[:4, :4])

# View with cell and gene names
print("\nCell names (first 4):")
print(adata.obs_names[:4])

print("\nGene names (first 4):")
print(adata.var_names[:4])

"""**Subset AnnData Object**"""

#  Create a subset of the data
adata_subset = adata[:4, :4]
print(adata_subset)

#  View as DataFrame
adata_subset.to_df()

"""**View Different Data Layers**"""

# If you have different data layers

if 'raw' in adata.layers:
    print("Raw counts (4x4):")
    print(adata.layers['raw'][:4, :4])

# View metadata
print("\nCell metadata (first 4 rows):")
print(adata.obs.head(4))

print("\nGene metadata (first 4 rows):")
print(adata.var.head(4))

# Normalized layers
adata.layers["log_transformed"] = np.log1p(adata.X)
adata
adata.X

"""**Conversion to DataFrames**"""

adata.to_df(layer="log_transformed")
adata.write('anndata_results.h5ad', compression="gzip")

"""**Unstructured data**"""

# AnnData has .uns, which allows for any unstructured metadata (lists or a dictionaries)
adata.uns["random"] = [1, 2, 3]
adata.uns

"""**Adding metadata**"""

obs_meta = pd.DataFrame({
        'time_yr': np.random.choice([0, 2, 4, 8], adata.n_obs),
        'subject_id': np.random.choice(['subject 1', 'subject 2', 'subject 4', 'subject 8'], adata.n_obs),
        'instrument_type': np.random.choice(['type a', 'type b'], adata.n_obs),
        'site': np.random.choice(['site x', 'site y'], adata.n_obs),
    },
    index=adata.obs.index,    # these are the same IDs of observations as above!
)

# Shape (rows, columns)
print("Shape:", obs_meta.shape)

# Column names
print("Columns:", obs_meta.columns.tolist())

# Row names (index)
print("First 5 row names:", obs_meta.index[:5].tolist())

# Data types
print("Data types:\n", obs_meta.dtypes)

# Basic info
obs_meta.info()

adata = ad.AnnData(adata.X, obs=obs_meta, var=adata.var)
print(adata)

# Indexing into AnnData will assume that integer arguments to [] behave like .iloc in pandas,
# whereas string arguments behave like .loc. AnnData always assumes string indices.

adata[:5, ['Gene_1', 'Gene_3']]
adata_subset

"""**Reading large data**"""

adata = ad.read_h5ad('anndata_results.h5ad', backed='r')

# Install all required packages
!pip install gdown torch pyro-ppl numpy scanpy scikit-learn anndata

"""**Interfacing pytorch models with anndata**"""

import gdown
import torch
import torch.nn as nn
import pyro
import pyro.distributions as dist
import numpy as np
import scanpy as sc
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from anndata.experimental.pytorch import AnnLoader

"""**VAE definition**"""

class MLP(nn.Module):
    def __init__(self, input_dim, hidden_dims, out_dim):
        super().__init__()

        modules = []
        for in_size, out_size in zip([input_dim]+hidden_dims, hidden_dims):
            modules.append(nn.Linear(in_size, out_size))
            modules.append(nn.LayerNorm(out_size))
            modules.append(nn.ReLU())
            modules.append(nn.Dropout(p=0.05))

        modules.append(nn.Linear(hidden_dims[-1], out_dim))
        self.fc = nn.Sequential(*modules)

    def forward(self, *inputs):
        input_cat = torch.cat(inputs, dim=-1)
        return self.fc(input_cat)

class CVAE(nn.Module):
    # The code is based on the scarches trVAE model
    # https://github.com/theislab/scarches/blob/v0.3.5/scarches/models/trvae/trvae.py
    # and on the pyro.ai Variational Autoencoders tutorial
    # http://pyro.ai/examples/vae.html
    def __init__(self, input_dim, n_conds, n_classes, hidden_dims, latent_dim):
        super().__init__()

        self.encoder = MLP(input_dim+n_conds, hidden_dims, 2*latent_dim) # output - mean and logvar of z

        self.decoder = MLP(latent_dim+n_conds, hidden_dims[::-1], input_dim)
        self.theta = nn.Linear(n_conds, input_dim, bias=False)

        self.classifier = nn.Linear(latent_dim, n_classes)

        self.latent_dim = latent_dim

    def model(self, x, batches, classes, size_factors):
        pyro.module("cvae", self)

        batch_size = x.shape[0]

        with pyro.plate("data", batch_size):
            z_loc = x.new_zeros((batch_size, self.latent_dim))
            z_scale = x.new_ones((batch_size, self.latent_dim))
            z = pyro.sample("latent", dist.Normal(z_loc, z_scale).to_event(1))

            classes_probs = self.classifier(z).softmax(dim=-1)
            pyro.sample("class", dist.Categorical(probs=classes_probs), obs=classes)

            dec_mu = self.decoder(z, batches).softmax(dim=-1) * size_factors[:, None]
            dec_theta = torch.exp(self.theta(batches))

            logits = (dec_mu + 1e-6).log() - (dec_theta + 1e-6).log()

            pyro.sample("obs", dist.NegativeBinomial(total_count=dec_theta, logits=logits).to_event(1), obs=x.int())

    def guide(self, x, batches, classes, size_factors):
        batch_size = x.shape[0]

        with pyro.plate("data", batch_size):
            z_loc_scale = self.encoder(x, batches)

            z_mu = z_loc_scale[:, :self.latent_dim]
            z_var = torch.sqrt(torch.exp(z_loc_scale[:, self.latent_dim:]) + 1e-4)

            pyro.sample("latent", dist.Normal(z_mu, z_var).to_event(1))

"""**Download the data**"""

url = 'https://drive.google.com/uc?id=1ehxgfHTsMZXy6YzlFKGJOsBKQ5rrvMnd'
output = 'pancreas.h5ad'
gdown.download(url, output, quiet=False)

!pwd
!ls -1
!ls -1 /content

adata = sc.read('pancreas.h5ad')

sc.pp.neighbors(adata)
sc.tl.umap(adata)

# We can see that the data has strong batch effects. We want to integrate the studies using our VAE model.

sc.pl.umap(adata, color=['study', 'cell_type'], wspace=0.35)

adata.X = adata.raw.X # put raw counts to .X

# For our model we need size factors (library sizes) for each cell for the means of negative binomial reconstruction loss.

adata.obs['size_factors'] = adata.X.sum(1)

# Here we set up the encoders for labels in our AnnData object.
# These encoders will be used by AnnLoader to convert the labels
# on the fly when they are accessed from the dataloader during the training phase.

adata.obs['study'].cat.categories

from sklearn.preprocessing import OneHotEncoder

# Assuming you want a non-sparse output
encoder_celltype = OneHotEncoder(sparse_output=False, dtype=np.float32)
encoder_celltype.fit(adata.obs['cell_type'].to_numpy()[:, None])

adata.obs['cell_type'].cat.categories

use_cuda = torch.cuda.is_available()

# You can create the converter with a function or a Mapping of functions which will be applied to the values of attributes (.obs, .obsm, .layers, .X) or
# to specific keys of these attributes in the subset object.
# Specify an attribute and a key (if needed) as keys of the passed Mapping and a function to be applied as a value.
# Here we define a converter which will transform the values of the keys 'study' and 'cell_type' of .obs using the encoders created above.

# Here we define a converter which will transform the values of the keys 'study' and 'cell_type' of .obs using the encoders created above.

encoders = {
    'obs': {
        'study': lambda s: encoder_study.transform(s.to_numpy()[:, None]),
        'cell_type': lambda s: encoder_celltype.transform(s.to_numpy()[:, None])
    }
}

# Note that if use_cuda=True, then all numeric values will be converted to tensors and sent to cuda,
# so you donâ€™t need to do any conversion during the training phase.

dataloader = AnnLoader(adata, batch_size=128, shuffle=True, convert=encoders, use_cuda=use_cuda)

dataloader.dataset

